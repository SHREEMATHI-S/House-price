# -*- coding: utf-8 -*-
"""House Price

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AUtcuaFZaKLyC2oaQ6PHYy64b8C3w-dO
"""

import numpy as np # linear algebra
import pandas as pd

from google.colab import files
uploads1 = files.upload()

import io

# Load each file into a pandas DataFrame using the uploaded content
ds = pd.read_csv(io.BytesIO(uploads1['data.csv']))

df = pd.read_csv(io.BytesIO(uploads1['data.csv']))
df.head()

df['Total_room'] = df['bedrooms'] + df['bathrooms']
df['Total_sqfeet'] = df['sqft_above'] + df['sqft_basement']
# Dropping 'City' and 'Country' columns
df = df.drop(columns=['sqft_lot', 'floors','waterfront','view','street','statezip','date','yr_built','yr_renovated','bedrooms','bathrooms','sqft_basement','sqft_above'])

df.info()

import pandas as pd
from sklearn.preprocessing import LabelEncoder
df_one_hot = pd.get_dummies(df[['city','country']])
# Drop original 'City' and 'Education' columns
df.drop(['city','country'], axis=1, inplace=True)

# Concatenate the original DataFrame with the one-hot encoded DataFrame
df = pd.concat([df, df_one_hot], axis=1)

print(df)

df.info()

import pandas as pd
from sklearn.preprocessing import StandardScaler

# Select only numerical columns
numerical_cols = df.select_dtypes(include=['number'])

# Initialize the scaler
scaler = StandardScaler()

# Fit and transform the numerical columns
df_scaled = scaler.fit_transform(numerical_cols)

# Convert the scaled data back to a DataFrame with the original column names
df_scaled = pd.DataFrame(df_scaled, columns=numerical_cols.columns)

# If you want to replace the original numerical columns with the scaled ones
df[numerical_cols.columns] = df_scaled

print(df)

df.head(5)

def remove_outliers(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    df = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]
    return df

# Apply outlier removal only to numeric columns
numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns

for column in numeric_cols:
    df = remove_outliers(df, column)

print(df)

from sklearn.model_selection import train_test_split
X = df.drop('price', axis=1)
y = df['price']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import accuracy_score
# Initialize the Random Forest Regressor
rf_Regressor = RandomForestRegressor(n_estimators=100, random_state=42)

# Train the classifier
rf_Regressor.fit(X_train, y_train)

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Make predictions with the trained model
y_pred = rf_Regressor.predict(X_test)

# Calculate the Mean Squared Error (MSE)
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse:.2f}')

# Calculate the Mean Absolute Error (MAE)
mae = mean_absolute_error(y_test, y_pred)
print(f'Mean Absolute Error: {mae:.2f}')

r2 = r2_score(y_test, y_pred)
print(f'R-squared: {r2:.2f}')

import joblib
joblib.dump(rf_Regressor, 'rf_Regressor.pkl')
rf_Regressor = joblib.load('rf_Regressor.pkl')

num_columns =len(df.columns)
print("Number of columns:", num_columns)

new_data = [[0.06, 18.0, 2.31, 0.0, 0.6, 5.0, 85.0, 1.0, 4.0, 307.0, 21.0,4.98,0.06, 18.0, 2.31, 0.0, 0.6, 5.0, 85.0, 1.0, 4.0, 307.0, 21.0, 396.90, 4.98,0.06, 18.0, 2.31, 0.0, 0.6, 5.0, 85.0, 1.0, 4.0, 307.0, 21.0, 396.90, 4.98,0.06, 18.0, 2.31, 0.0, 0.6, 5.0, 85.0, 1.0, 4.0, 307.0, 21.0]]
prediction = rf_Regressor.predict(new_data)
print(prediction)

df.isna().sum()

df.shape

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import seaborn as sns
import matplotlib.pyplot as plt
import sklearn
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
plt.figure(figsize=(12,8))
outlier_checker=sns.boxplot(df)
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Sample Data (Replace this with your actual data)
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

# Dummy Data
x = np.random.rand(100, 1) * 100
y = 5 + 2 * x + np.random.randn(100, 1) * 10

# Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1)

# Train the model
model = LinearRegression()
model.fit(X_train, y_train)
plt.figure(figsize=(10, 6))
sns.scatterplot(x=X_train.ravel(), y=y_train.ravel(), color='blue', label='Train Data')
sns.lineplot(x=X_train.ravel(), y=model.predict(X_train).ravel(), color='red', label='Regression Line')
plt.xlabel('Feature')
plt.ylabel('Target')
plt.title('Regression Line Plot')
plt.legend()
plt.show()

plt.figure(figsize=(10, 6))
sns.histplot(df['price'], kde=True, bins=30)
plt.title('Distribution of House Prices')
plt.xlabel('Price')
plt.ylabel('Frequency')
plt.show()

numeric_features = df.select_dtypes(include=['int64', 'float64']).columns
corr_matrix = df[numeric_features].corr()

# Plot heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap='coolwarm')
plt.title('Correlation Heatmap')
plt.show()